{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import plotly.express as px\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split as ttsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentiel multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance(x0,y0,x,y):\n",
    "    return np.sqrt((x0-x)*(x0-x)+(y-y0)*(y-y0))\n",
    "\n",
    "def potV(x0,y0,r,x,y,a):\n",
    "    dis = distance(x0,y0,x,y)\n",
    "    print(dis)\n",
    "    if dis<=r:\n",
    "        val = -a*(1-abs(x-x0)/r)*(1-abs(x-x0)/r)-a*(1-abs(y-y0)/r)*(1-abs(y-y0)/r)\n",
    "        return val\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "N=1000\n",
    "\n",
    "class MultimodalPotential:\n",
    "    def __init__(self,bowls_coord, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param bowls_coord: matrix, where bowls_coord[i]=([x0,y0,r,a])\n",
    "        where (x0,y0) are the coords of the center of the bowl and r its radius\n",
    "        a the intensity of the well of potential made by the bowl\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self._bowls_coord=np.copy(bowls_coord)\n",
    "        print(self._bowls_coord)\n",
    "        self.dim = 2\n",
    "        self.nbr_bowls = np.shape(bowls_coord)\n",
    "        self.Z = None\n",
    "\n",
    "    def V(self, X):\n",
    "        \"\"\"Potential function\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        V=0\n",
    "        bowl=self._bowls_coord\n",
    "        for bowl in self._bowls_coord:\n",
    "            x0=bowl[0]\n",
    "            y0=bowl[1]\n",
    "            r=bowl[2]\n",
    "            a=bowl[3]\n",
    "            dis = np.sqrt((x0-x)*(x0-x)+(y-y0)*(y-y0))\n",
    "            V-=a*np.exp(-dis*dis/(r*r))\n",
    "        V+=0.2 * (x ** 4) + 0.2 * ((y - 1/3) ** 4)\n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"\n",
    "        dVx = 0\n",
    "        for bowl in self._bowls_coord:\n",
    "            x0=bowl[0]\n",
    "            y0=bowl[1]\n",
    "            r=bowl[2]\n",
    "            a=bowl[3]\n",
    "            dis = np.sqrt((x0-x)*(x0-x)+(y-y0)*(y-y0))\n",
    "            dVx+=2*a*(x-x0)*np.exp(-dis*dis/(r*r))/(r*r)\n",
    "\n",
    "        dVx+= 0.8 * (x**3)\n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"\n",
    "        dVy = 0\n",
    "        for bowl in self._bowls_coord:\n",
    "            x0=bowl[0]\n",
    "            y0=bowl[1]\n",
    "            r=bowl[2]\n",
    "            a=bowl[3]\n",
    "            dis = np.sqrt((x0-x)*(x0-x)+(y-y0)*(y-y0))\n",
    "            dVy+=2*a*(y-y0)*np.exp(-dis*dis/(r*r))/(r*r)\n",
    "\n",
    "        dVy+= 0.8 * (y-y0)**3\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentiel sous-variété"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta(X):\n",
    "    x=X[0]\n",
    "    y=X[1]\n",
    "    if x>0 and y>=0:\n",
    "        return np.arctan(y/x)\n",
    "    if x>0 and y<0:\n",
    "        return np.arctan(y/x)+2*np.pi\n",
    "    if x<0:\n",
    "        return np.arctan(y/x) + np.pi\n",
    "    if x==0 and y>0:\n",
    "        return np.pi/2\n",
    "    else :\n",
    "        return 3*np.pi/2 \n",
    "        \n",
    "class Subvaraitiespotential:\n",
    "    def __init__(self,A, rs,hs,i,beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param A=Amplitude\n",
    "        \"\"\"\n",
    "        self.Ampl=A\n",
    "        self.R=rs\n",
    "        self.length=hs\n",
    "        self.angle=i\n",
    "        self.beta = beta\n",
    "\n",
    "\n",
    "    def V(self, X):\n",
    "        \"\"\"Potential function\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        r=np.sqrt(x**2 + y**2)\n",
    "        t = theta(X)\n",
    "        V=self.Ampl*r*np.exp(-r/self.length)*np.cos((np.log(r/self.R)/np.tan(self.angle)-t))\n",
    "        return V\n",
    "\n",
    "    def dV_r(self, X):\n",
    "        \"\"\"Potential function\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, derivative of potential energy value in r\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        r=np.sqrt(x**2 + y**2)\n",
    "        t = theta(X)\n",
    "        dV=self.Ampl*np.exp(-r/self.length)*np.cos((np.log(r/self.R)/np.tan(self.angle)-t))*(1-r/self.length)-self.Ampl*np.exp(-r/self.length)*np.sin((np.log(r/self.R)/np.tan(self.angle)-t))/np.tan(self.angle)\n",
    "        return dV\n",
    "\n",
    "    def dV_theta(self, X):\n",
    "        \"\"\"Potential function\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, derivative of potential energy value in r\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        r=np.sqrt(x**2 + y**2)\n",
    "        t = theta(X)\n",
    "        dV=self.Ampl*r*np.exp(-r/self.length)*np.sin((np.log(r/self.R)/np.tan(self.angle)-t))\n",
    "        return dV\n",
    "\n",
    "    def dV_x(self, x,y):\n",
    "        X = np.array([x, y])\n",
    "        r=np.sqrt(x**2 + y**2)\n",
    "        t = theta(X)\n",
    "        dVdx= (self.dV_r(X)- self.dV_theta(X)/r)*np.cos(t)\n",
    "        return dVdx\n",
    "\n",
    "    def dV_y(self, x,y):\n",
    "        X = np.array([x, y])\n",
    "        r=np.sqrt(x**2 + y**2)\n",
    "        t = theta(X)\n",
    "        dVdy= (self.dV_r(X) + self.dV_theta(X)/r)*np.sin(t)\n",
    "        return dVdy\n",
    "\n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TriplewellPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleWellPotential:\n",
    "    \"\"\"Class to gather methods related to the potential function\"\"\"\n",
    "    def __init__(self, beta):\n",
    "        \"\"\"Initialise potential function class\n",
    "\n",
    "        :param beta: float,  inverse temperature = 1 / (k_B * T)\n",
    "        :param Z: float, partition function (computed below)\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.dim = 2\n",
    "        self.Z = None\n",
    "        \n",
    "    def V(self, X):\n",
    "        \"\"\"Potential fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: V: float, potential energy value\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        u = g(x) * (g(y - 1/3) - g(y - 5/3))\n",
    "        v = g(y) * (g(x - 1) + g(x + 1))\n",
    "        V = 3 * u - 5 * v + 0.2 * (x ** 4) + 0.2 * ((y - 1/3) ** 4)\n",
    "        return V\n",
    "    \n",
    "    def dV_x(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVx: float, derivative of the potential with respect to x\n",
    "        \"\"\"\n",
    "        u = g(x) * (g(y - 1/3) - g(y - 5/3))\n",
    "        a = g(y) * ((x - 1)*g(x - 1) + (x + 1) * g(x + 1))\n",
    "        dVx = -6 * x * u + 10 * a + 0.8 * (x ** 3)\n",
    "        return dVx\n",
    "    \n",
    "    def dV_y(self, x, y):\n",
    "        \"\"\"\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: dVy: float, derivative of the potential with respect to y\n",
    "        \"\"\"\n",
    "        u = g(x) * ((y - 1/3) * g(y - 1/3) - (y - 5/3) * g(y - 5/3))\n",
    "        b = g(y) * (g(x - 1) + g(x + 1))\n",
    "        dVy = -6 * u + 10 * y * b + 0.8 * ((y - 1/3) ** 3)\n",
    "        return dVy\n",
    "    \n",
    "    def nabla_V(self, X):\n",
    "        \"\"\"Gradient of the potential energy fuction\n",
    "\n",
    "        :param X: np.array, Position  vector (x,y), ndim = 1, shape = (2,)\n",
    "        :return: grad(X): np.array, gradient with respect to position vector (x,y), ndim = 1, shape = (2,)\n",
    "        \"\"\"\n",
    "        assert(type(X) == np.ndarray)\n",
    "        assert(X.ndim == 1)\n",
    "        assert(X.shape[0] == 2)\n",
    "        return np.array([self.dV_x(X[0], X[1]), self.dV_y(X[0], X[1])])\n",
    "        \n",
    "    def boltz_weight(self, x, y):\n",
    "        \"\"\"Compute the unnormalized weight of a configuration according to the Boltzmann distribution\n",
    "\n",
    "        :param x: float, x coordinate\n",
    "        :param y: float, y coordinate\n",
    "\n",
    "        :return: normalized Blotzmann weight\n",
    "        \"\"\"\n",
    "        X = np.array([x, y])\n",
    "        return np.exp(-self.beta * self.V(X))\n",
    "    \n",
    "    def set_Z(self):\n",
    "        \"\"\"Partition function to normalize probability densities\n",
    "        \"\"\"\n",
    "        self.Z, _ = integrate.dblquad(self.boltz_weight, -5, 5, -5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(potential):\n",
    "    if potential ==\"multimodal\":\n",
    "        bowls = np.array([[0.5,0.5,0.15,0.2],[0.7,0.87,0.1,1.5],[0.2,0.8,0.1,0.5]])\n",
    "        Potential = MultimodalPotential(bowls)\n",
    "        X=np.zeros((N,N))\n",
    "        Y=np.zeros((N,N))\n",
    "        Potential_map=np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                X[i,j]=i/N\n",
    "                Y[i,j]=j/N\n",
    "                Potential_map[i,j]=Potential.V(np.array([i/N,j/N]))\n",
    "    else :\n",
    "        Potential = Subvaraitiespotential(1, 1e-3,2,math.pi/10, 4)\n",
    "    \n",
    "        grid = np.linspace(-2,2,100)\n",
    "\n",
    "        X=np.outer(grid, np.ones(100))\n",
    "        Y=np.outer(grid + 0.5, np.ones(100)).T\n",
    "        Potential_map=np.zeros([100, 100])\n",
    "        for i in range(100):\n",
    "            for j in range(100):\n",
    "                    Potential_map[i,j]=Potential.V(np.array([grid[i],grid[j]+0.5]))\n",
    "\n",
    "    \n",
    "    fig= plt.figure(figsize=(9,3))\n",
    "    ax0 = fig.add_subplot(1,2,1, projection='3d')\n",
    "    ax1 = fig.add_subplot(1,2,2)\n",
    "    # Plot the surface\n",
    "    ax0.plot_surface(X, Y, Potential_map, color='b')\n",
    "    ax1.pcolormesh(X, Y, Potential_map, cmap='coolwarm_r',shading='auto')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_plots(Potential):\n",
    "\n",
    "    grid = np.linspace(-2,2,100)\n",
    "    X = np.outer(grid, np.ones(100))\n",
    "    Y = np.outer(grid + 0.5, np.ones(100)).T\n",
    "    potential_on_grid = np.zeros([100, 100])\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            potential_on_grid[i, j] = Potential.V(np.array([grid[i], grid[j] + 0.5]))\n",
    "\n",
    "    fig3D = go.Figure(data=[go.Surface(z=potential_on_grid, x=X, y=Y)])\n",
    "    fig3D.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                  highlightcolor=\"limegreen\", project_z=True))\n",
    "    fig3D.update_layout(title='The potential map', autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90)       \n",
    "    )\n",
    "\n",
    "    return fig3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define a function 'UnbiasedTraj' to generate an trajectory according an Euler--Maruyama discretization \n",
    "$$\n",
    "X^{n+1} = X^n - \\Delta t \\nabla V(X^n) + \\sqrt{\\frac{2 \\Delta t}{\\beta}} \\, G^n \n",
    "$$\n",
    "of the overdamped Langevin dynamics\n",
    "$$\n",
    "dX_t = -\\nabla V(X_t) \\, dt + \\sqrt{\\frac{2}{\\beta}} \\, dW_t\n",
    "$$\n",
    "This functions takes as argument a potential object, initial conditions, the number of simulation steps and a time step. It generates a realization of a trajectory (subsampled at some prescribed rate), and possibly records the value of the potential energy function at the points along the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnbiasedTraj(pot, X_0, delta_t=1e-3, T=3000, save=1, save_energy=False, seed=0):\n",
    "    \"\"\"Simulates an overdamped langevin trajectory with a Euler-Maruyama numerical scheme \n",
    "\n",
    "    :param pot: potential object, must have methods for energy gradient and energy evaluation\n",
    "    :param X_0: Initial position, must be a 2D vector\n",
    "    :param delta_t: Discretization time step\n",
    "    :param T: Number of points in the trajectory (the total simulation time is therefore T * delta_t)\n",
    "    :param save: Integer giving the period (counted in number of steps) at which the trajectory is saved\n",
    "    :param save_energy: Boolean parameter to save energy along the trajectory\n",
    "\n",
    "    :return: traj: np.array with ndim = 2 and shape = (T // save + 1, 2)\n",
    "    :return: Pot_values: np.array with ndim = 2 and shape = (T // save + 1, 1)\n",
    "    \"\"\"\n",
    "    r = np.random.RandomState(seed)\n",
    "    X = X_0\n",
    "    dim = X.shape[0]\n",
    "    traj = [X]\n",
    "    if save_energy:\n",
    "        Pot_values = [pot.V(X)]\n",
    "    else:\n",
    "        Pot_values = None\n",
    "    for i in range(T):\n",
    "        b = r.normal(size=(dim,))\n",
    "        X = X - pot.nabla_V(X) * delta_t + np.sqrt(2 * delta_t/pot.beta) * b\n",
    "        if X[0]>2 or X[0]<-2:\n",
    "            X[0]=2*np.sign(X[0])\n",
    "        if X[1]>2 or X[1]<-2:\n",
    "            X[1]=2*np.sign(X[1])\n",
    "        if i % save==0:\n",
    "            traj.append(X)\n",
    "            if save_energy:\n",
    "                Pot_values.append(pot.V(X))\n",
    "    return np.array(traj), np.array(Pot_values)\n",
    "\n",
    "def plot_trajectory(Potential):\n",
    "    grid = np.linspace(-2,2,100)\n",
    "    x_plot = np.outer(grid, np.ones(100))\n",
    "    y_plot = np.outer(grid + 0.5, np.ones(100)).T\n",
    "    potential_on_grid = np.zeros([100, 100])\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            potential_on_grid[i, j] = Potential.V(np.array([grid[i], grid[j] + 0.5]))\n",
    "\n",
    "    \n",
    "    delta_t = 0.01\n",
    "    T = 1000\n",
    "    x_0 = np.array([-0.177, -0.5753])\n",
    "    trajectory, _ = UnbiasedTraj(Potential, x_0, delta_t=delta_t, T=T, save=1, save_energy=False, seed=None)\n",
    "    fig = plt.figure(figsize=(9,3))\n",
    "    ax0 = fig.add_subplot(1, 2, 1)\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax0.pcolormesh(x_plot,y_plot,  potential_on_grid, cmap='coolwarm_r', shading='auto')\n",
    "    ax0.scatter(trajectory[:,0], trajectory[:,1], marker='x')\n",
    "    ax1.plot(range(len(trajectory[:,0])), trajectory[:,0], label='x coodinate along trajectory')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a model for an autoencoder with PyTorch. You can look up the various functions/classes below in the PyTorch documentation https://pytorch.org/docs/stable/index.html\n",
    "We consider two models: \n",
    "- SimpleAutoEncoder which corresponds to the simplest possible autoencoder going directly from the input to the bottleneck and then to the output\n",
    "- DeepAutoEncoder which allows to consider more hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,bottleneck_dim):\n",
    "        \"\"\"Initialise simplest autoencoder (input->bottleneck->ouput), with hyperbolic tangent activation function\n",
    "       \n",
    "        :param input_dim: int, Number of dimension of the input vectors\n",
    "        :param bottleneck_dim: int, Number of dimension of the bottleneck\n",
    "        \"\"\"\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, bottleneck_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bottleneck_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class DeepAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, bottleneck_dim):\n",
    "        \"\"\"Initialise auto encoder with hyperbolic tangent activation function\n",
    "        You can uncomment certain lines in the encoder and decoder functions to modify the topology of the network\n",
    "        Make sure when you initialise the AE object that the list 'hidden_dims' has a length consistent with the architecture\n",
    "\n",
    "        :param input_dim: int, Number of dimension of the input vectors\n",
    "        :param hidden_dims: list, List of hidden layers\n",
    "        :param bottleneck_dim: int, Number of dimension of the bottleneck\n",
    "        \"\"\"\n",
    "        super(DeepAutoEncoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dims[0]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], bottleneck_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bottleneck_dim, hidden_dims[-1]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-1], hidden_dims[-2]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[-2], hidden_dims[-3]),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_dims[0], input_dim),\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        # Input Linear function\n",
    "        encoded = self.encoder(inp)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to set the learning parameters for training neural networks: learning rate, loss function, optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_learning_parameters(model, learning_rate, loss='MSE', optimizer='Adam'):\n",
    "    \"\"\"Function to set learning parameter\n",
    "\n",
    "    :param model: Neural network model build with PyTorch,\n",
    "    :param learning_rate: Value of the learning rate\n",
    "    :param loss: String, type of loss desired ('MSE' by default, another choice leads to cross entropy)\n",
    "    :param optimizer: String, type of optimizer ('Adam' by default, another choice leads to SGD)\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #--- chosen loss function ---\n",
    "    if loss == 'MSE':\n",
    "        loss_function = nn.MSELoss()\n",
    "    else:\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "    #--- chosen optimizer ---\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    return loss_function, optimizer\n",
    "\n",
    "def train_AE(model, loss_function, optimizer, traj, weights, num_epochs=10, batch_size=32, test_size=0.2):\n",
    "    \"\"\"Function to train an AE model\n",
    "\n",
    "    :param model: Neural network model built with PyTorch,\n",
    "    :param loss_function: Function built with PyTorch tensors or built-in PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer object\n",
    "    :param traj: np.array, physical trajectory (in the potential pot), ndim == 2, shape == T // save + 1, pot.dim\n",
    "    :param weights: np.array, weights of each point of the trajectory when the dynamics is biased, ndim == 1, shape == T // save + 1, 1\n",
    "    :param num_epochs: int, number of times the training goes through the whole dataset\n",
    "    :param batch_size: int, number of data points per batch for estimation of the gradient\n",
    "    :param test_size: float, between 0 and 1, giving the proportion of points used to compute test loss\n",
    "\n",
    "    :return: model, trained neural net model\n",
    "    :return: training_data, list of lists of train losses and test losses; one per batch per epoch\n",
    "    \"\"\"\n",
    "    #--- prepare the data ---\n",
    "    # split the dataset into a training set (and its associated weights) and a test set\n",
    "    X_train, X_test, w_train, w_test = ttsplit(traj, weights, test_size=test_size)\n",
    "    X_train = torch.tensor(X_train.astype('float32'))\n",
    "    X_test = torch.tensor(X_test.astype('float32'))\n",
    "    w_train = torch.tensor(w_train.astype('float32'))\n",
    "    w_test = torch.tensor(w_test.astype('float32'))\n",
    "    # intialization of the methods to sample with replacement from the data points (needed since weights are present)\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(w_train, len(w_train))\n",
    "    test_sampler  = torch.utils.data.WeightedRandomSampler(w_test, len(w_test))\n",
    "    # method to construct data batches and iterate over them\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=X_train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=train_sampler)\n",
    "    test_loader  = torch.utils.data.DataLoader(dataset=X_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               sampler=test_sampler)\n",
    "    \n",
    "    #--- start the training over the required number of epochs ---\n",
    "    training_data = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model by going through the whole dataset\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for iteration, X in enumerate(train_loader):\n",
    "            # Set gradient calculation capabilities\n",
    "            X.requires_grad_()\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output\n",
    "            out = model(X)\n",
    "            # Evaluate loss\n",
    "            loss = loss_function(out, X)\n",
    "            # Store loss\n",
    "            train_loss.append(loss)\n",
    "            # Get gradient with respect to parameters of the model\n",
    "            loss.backward()\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "        # Evaluate the test loss on the test dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = []\n",
    "            for iteration, X in enumerate(test_loader):\n",
    "                out = model(X)\n",
    "                # Evaluate loss\n",
    "                loss = loss_function(out, X)\n",
    "                # Store loss\n",
    "                test_loss.append(loss)\n",
    "            training_data.append([torch.tensor(train_loss), torch.tensor(test_loss)])\n",
    "    return model, training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define the function to obtain the output of the encoder given the configuration of the system and the one for the gradient of the encoder with respect to the input values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xi_ae(model,  x):\n",
    "    \"\"\"Collective variable defined through an auto encoder model\n",
    "\n",
    "    :param model: Neural network model build with PyTorch\n",
    "    :param x: np.array, position, ndim = 2, shape = (1,1)\n",
    "\n",
    "    :return: xi: np.array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor(x.astype('float32'))\n",
    "    return model.encoder(x).detach().numpy()\n",
    "\n",
    "def grad_xi_ae(model, x):\n",
    "    \"\"\"Gradient of the collective variable defined through an auto encoder model\n",
    "\n",
    "    :param model: Neural network model build with pytorch,\n",
    "    :param x: np.array, position, ndim = 2, shape = (1,1)\n",
    "\n",
    "    :return: grad_xi: np.array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor(x.astype('float32'))\n",
    "    x.requires_grad_()\n",
    "    enc = model.encoder(x)\n",
    "    grad = torch.autograd.grad(enc, x)[0]\n",
    "    return grad.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define functions to train our autoencoders and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ae0, ae1, learning_rate, batch_size, num_epochs, loss, optimizer, trajectory):\n",
    "    \"\"\"\n",
    "        :param potential:  MultimodalPotential, potential\n",
    "        :param trajectory: np.array\n",
    "\n",
    "        :return: Fig1: Figure, training and test losses plots\n",
    "        :return: Fig2: Figure, Collective variables plots\n",
    "        \"\"\"\n",
    "   \n",
    "    loss_function, optimizer = set_learning_parameters(ae0, learning_rate=learning_rate, loss=loss, optimizer=optimizer)\n",
    "    ae0, training_data0 = train_AE(ae0,\n",
    "                                loss_function,\n",
    "                                optimizer,\n",
    "                                trajectory,\n",
    "                                np.ones(trajectory.shape[0]),\n",
    "                                batch_size=batch_size,\n",
    "                                num_epochs=num_epochs\n",
    "                                )\n",
    "    loss_function, optimizer = set_learning_parameters(ae1, learning_rate=learning_rate, loss=loss, optimizer=optimizer)\n",
    "    ae1, training_data1 = train_AE(ae1,\n",
    "                                loss_function,\n",
    "                                optimizer,\n",
    "                                trajectory,\n",
    "                                np.ones(trajectory.shape[0]),\n",
    "                                batch_size=batch_size,\n",
    "                                num_epochs=num_epochs\n",
    "                                )\n",
    "\n",
    "    #--- plot the evolution of the losses ---\n",
    "    loss_evol0 = []\n",
    "    loss_evol1 = []\n",
    "    # obtain average losses on each epoch by averaging the losses from each batch\n",
    "    for i in range(len(training_data1)):\n",
    "        loss_evol0.append([torch.mean(training_data0[i][0]), torch.mean(training_data0[i][1])])\n",
    "        loss_evol1.append([torch.mean(training_data1[i][0]), torch.mean(training_data1[i][1])])\n",
    "    loss_evol0 = np.array(loss_evol0)\n",
    "    loss_evol1 = np.array(loss_evol1)\n",
    "\n",
    "    # plot these average losses\n",
    "    fig1, (ax0, ax1)  = plt.subplots(1,2, figsize=(10,4)) \n",
    "    ax0.plot(loss_evol0[:, 0], '--', label='train loss', marker='x')\n",
    "    ax0.plot(range(1, len(loss_evol0[:, 1])), loss_evol0[: -1, 1], '-.', label='test loss', marker='+')\n",
    "    ax0.legend()\n",
    "    ax0.set_title(\"Average losses for the simple AE\")\n",
    "    ax1.plot(loss_evol1[:, 0], '--', label='train loss', marker='x')\n",
    "    ax1.plot(range(1, len(loss_evol1[:, 1])), loss_evol1[: -1, 1], '-.', label='test loss', marker='+')\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\" Average losses for the Deep AE\")\n",
    "\n",
    "    return fig1\n",
    "    \n",
    "def plot_results(ae0, ae1, potential):  \n",
    "    \n",
    "    \n",
    "    #--- plot the contour lines of the AE functions ---\n",
    "    # construct the grid\n",
    "    grid = np.linspace(-2,2,100)\n",
    "    x_plot = np.outer(grid, np.ones(100))\n",
    "    y_plot = np.outer(grid + 0.5, np.ones(100)).T\n",
    "    potential_on_grid = np.zeros([100, 100])\n",
    "    xi_ae0_on_grid = np.zeros([100, 100])\n",
    "    xi_ae1_on_grid = np.zeros([100, 100])\n",
    "    #bars= np.zeros(100)\n",
    "    # compute values of potential and AEs on the grid\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            x = np.array([grid[i], grid[j] + 0.5])\n",
    "            potential_on_grid[i, j] = potential.V(x)\n",
    "            xi_ae0_on_grid[i,j] = xi_ae(ae0, x)\n",
    "            xi_ae1_on_grid[i,j] = xi_ae(ae1, x)\n",
    "    # superimpose contour plots to colormap of the potential\n",
    "    fig2, (ax0, ax1)  = plt.subplots(1,2, figsize=(9,3))        \n",
    "    ax0.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "    ax0.contour(x_plot, y_plot, xi_ae0_on_grid, 20, cmap = 'viridis')\n",
    "    ax0.set_title(\"Reaction coordinates with the simple AE\")\n",
    "    ax1.pcolormesh(x_plot, y_plot, potential_on_grid, cmap='coolwarm_r',shading='auto')\n",
    "    ax1.contour(x_plot, y_plot, xi_ae1_on_grid, 20, cmap = 'viridis')\n",
    "    ax1.set_title(\"Reaction coordinates with the Deep AE\")\n",
    "    return fig2\n",
    "\n",
    "def plot_hist(autoencoder, traj):\n",
    "    \"\"\"Plot the histogram\n",
    "    :param autoencoder: model of neural network\n",
    "                  traj: trajectory with wich we evaluate the encoder array of shape (time, 2)\n",
    "    :return histogram of the collective variable returned by autoencoder for each position in traj\n",
    "    \"\"\"\n",
    "    cv = []\n",
    "    #compute cv\n",
    "    for x in traj:\n",
    "        cv.append(xi_ae(autoencoder,  x)[0])\n",
    "        \n",
    "    #plot_hist\n",
    "    fig, axis = plt.subplots(figsize =(10, 5))\n",
    "    axis.hist(cv, edgecolor = 'grey', alpha=0.5, color='blue')\n",
    "    plt.title('Histogram')\n",
    "    plt.xlabel('CV obtained by the encoder')\n",
    "    plt.ylabel('Effective')\n",
    "    plt.style.use('seaborn-ticks')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to create some potentials and plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=6\n",
    "bowls = np.array([[-0.7,-0.7,0.5,2],[0.7,0.7,0.5,2], [0,0,0.3,1]])\n",
    "Potential = MultimodalPotential(bowls,beta)\n",
    "\n",
    "fig_pot=create_plots(Potential)\n",
    "fig_pot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid = np.linspace(-2,2,100)\n",
    "\n",
    "X=np.outer(grid, np.ones(100))\n",
    "Y=np.outer(grid + 0.5, np.ones(100)).T\n",
    "Potential_map=np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "            Potential_map[i,j]=Potential.V(np.array([grid[i],grid[j]+0.5]))\n",
    "\n",
    "fig= plt.figure(figsize=(9,3))\n",
    "ax0 = fig.add_subplot(1,2,1, projection='3d')\n",
    "ax1 = fig.add_subplot(1,2,2)\n",
    "# Plot the surface\n",
    "ax0.plot_surface(X, Y, Potential_map, color='b')\n",
    "ax1.pcolormesh(X, Y, Potential_map, cmap='coolwarm_r',shading='auto')\n",
    "grid = np.linspace(-2,2,100)\n",
    "x_plot = np.outer(grid, np.ones(100))\n",
    "y_plot = np.outer(grid + 0.5, np.ones(100)).T\n",
    "potential_on_grid = np.zeros([100, 100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        potential_on_grid[i, j] = Potential.V(np.array([grid[i], grid[j] + 0.5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 0.01\n",
    "T = 40000\n",
    "x_0 = np.array([0, 0])\n",
    "#trajectory, _ = pt.UnbiasedTraj(Potential, x_0, delta_t=delta_t, T=T, save=1, save_energy=False, seed=None)\n",
    "trajectory=np.loadtxt('traj_bowl3_3.csv', delimiter=',')\n",
    "fig0 = plt.figure(figsize=(9,3))\n",
    "ax0 = fig.add_subplot(1, 2, 1)\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax0.pcolormesh(x_plot,y_plot,  potential_on_grid, cmap='coolwarm_r', shading='auto')\n",
    "ax0.scatter(trajectory[:,0], trajectory[:,1], marker='x')\n",
    "ax1.plot(range(len(trajectory[:,0])), trajectory[:,0], label='x coodinate along trajectory')\n",
    "#np.savetxt('traj_bowl3_5.csv', trajectory, delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "batch_size = 100\n",
    "num_epochs = 500\n",
    "loss = 'MSE'\n",
    "optimizer = 'Adam'\n",
    "ae0 = SimpleAutoEncoder(2,1) \n",
    "ae = DeepAutoEncoder(2, [4,8,2], 1)\n",
    "fig2=train(ae0, ae, learning_rate, batch_size, num_epochs, loss, optimizer, trajectory)\n",
    "#ae = torch.load('AE_model_sub4')\n",
    "fig3=plot_results(ae0, ae,Potential)\n",
    "torch.save(ae, 'AE_model_mod3_2')\n",
    "\n",
    "fig4 = plot_hist(ae, trajectory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
